# Foundation models definition
> Models trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks[^1]
# Training
Using:
1. Self-supervises learning
2. Scale

# Foundation models
1. BERT
2. RoBERTa
3. GPT-2
4. T5
5. Turing NLG
6. GPT-3

| Organization | Models |
|---|---|
| OpenAI | GPT-3, Codex, DALL-E, CLIP |
| Meta | OPT |
| AI21labs | Jurassic |
| Huggingface | |
| HuggingFace + BigSciece | BLOOM |
| Nvidia + Microsoft | MT-NLG |
| Stability.ai | Stable diffusion |
| BAAI | Wu Dao 2.0 |
| EleutherAI | GPT-NeoX |
| DeepMind | Gopher, Chinchilla |
| Huawei | PanGu-Alpha |
| Naver | HyperCL, OVA |
| Google | PaLM, MUM |
[^1]: ["Introducing the Center for Research on Foundation Models (CRFM)"](https://hai.stanford.edu/news/introducing-center-research-foundation-models-crfm) Stanford HAI. Retrieved 11 June 2022.
