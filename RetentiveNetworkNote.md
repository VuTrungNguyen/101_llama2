# Retentive Network: A successor to Transformer for LLM
- Current architecture:
  - Linear Transformer
  - Recurrent Network
  - Transformer
- Current performance metrics:
  - Low-cost Inference
  - Training Parallelism
  - Strong Performance
 
# Transformers thinking
- RASP (Restricted Access Sequence Process)
  - Transformer-encoder is a sequence to sequence function (Sequence Operator - s-op)
  - Layers apply operations to the sequence
  - RASP:
    - Input sequence
    - What layers can do with Input
